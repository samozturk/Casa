{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f289dd8f",
   "metadata": {},
   "source": [
    "## Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66690c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRACING: true\n",
      "PROJECT: langgraph-debug\n",
      "API KEY SET: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7j/qr6tvgr97s3b65k1tx46fry80000gn/T/ipykernel_21765/2253802776.py:38: ResourceWarning: unclosed <socket.socket fd=88, family=2, type=1, proto=6, laddr=('127.0.0.1', 50570), raddr=('127.0.0.1', 11434)>\n",
      "  llm_with_tools = llm.bind_tools([duck, dog])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, List, Any\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langgraph-debug\"\n",
    "\n",
    "print(\"TRACING:\", os.environ.get(\"LANGCHAIN_TRACING_V2\"))\n",
    "print(\"PROJECT:\", os.environ.get(\"LANGCHAIN_PROJECT\"))\n",
    "print(\"API KEY SET:\", bool(os.environ.get(\"LANGCHAIN_API_KEY\")))\n",
    "\n",
    "@tool\n",
    "def duck() -> str:\n",
    "    \"\"\"Make a duck sound.\"\"\"\n",
    "    return \"quack\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def dog() -> str:\n",
    "    \"\"\"Make a dog sound.\"\"\"\n",
    "    return \"woff\"\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8B\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "llm_with_tools = llm.bind_tools([duck, dog])\n",
    "\n",
    "class State(TypedDict):\n",
    "    user_text: str\n",
    "    messages: List[Any]\n",
    "\n",
    "\n",
    "def llm_node(state: State) -> dict:\n",
    "    messages = state.get(\"messages\", [])\n",
    "    user_text = state.get(\"user_text\")\n",
    "\n",
    "    # First turn: inject HumanMessage exactly once\n",
    "    if not messages:\n",
    "        if user_text is None:\n",
    "            raise ValueError(\"user_text missing\")\n",
    "        messages = [HumanMessage(content=user_text)]\n",
    "\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "\n",
    "    return {\n",
    "        # ðŸ”‘ APPEND â€” never replace\n",
    "        \"messages\": messages + [response]\n",
    "    }\n",
    "\n",
    "tool_node = ToolNode([duck, dog])\n",
    "\n",
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_node(\"llm\", llm_node)\n",
    "graph.add_node(\"tools\", tool_node)\n",
    "\n",
    "graph.set_entry_point(\"llm\")\n",
    "\n",
    "# Decide whether to run tools or stop\n",
    "graph.add_conditional_edges(\n",
    "    \"llm\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# After tools run, we stop (simple example)\n",
    "graph.add_edge(\"tools\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "result = app.invoke({\n",
    "    \"user_text\": \"I wonder what this animal with four leg says.\",\n",
    "    \"messages\": []\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2926765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_text': 'I wonder what this animal with four leg says.',\n",
       " 'messages': [ToolMessage(content='woff', name='dog', tool_call_id='a0b852d5-e9a0-4cfb-bb5c-5f53b8cceffe')]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab860f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450f1f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3633513",
   "metadata": {},
   "source": [
    "## Intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "74edbb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7j/qr6tvgr97s3b65k1tx46fry80000gn/T/ipykernel_21765/1876980877.py:6: ResourceWarning: unclosed <socket.socket fd=90, family=2, type=1, proto=6, laddr=('127.0.0.1', 60424), raddr=('127.0.0.1', 11434)>\n",
      "  llm = ChatOllama(\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, List, Literal\n",
    "from langchain_core.messages import BaseMessage, SystemMessage\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1:14B\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "class Rooms(BaseModel):\n",
    "    rooms: Literal[\"office\", \"bedroom\"]\n",
    "    topics: Literal[\"home/lights/office\", \"home/lights/bedroom\"]\n",
    "    status: Literal[\"off\", \"on\"]\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    intent: str | None\n",
    "    policy_check: bool\n",
    "    rooms: Rooms | None\n",
    "\n",
    "config = {\n",
    "    \"bedroom\": \"home/lights/bedroom\",\n",
    "    \"office\": \"home/lights/office\"\n",
    "}\n",
    "\n",
    "INTENT_PROMPT = SystemMessage(\n",
    "    content=(\n",
    "        \"Classify the user's intent.\\n\\n\"\n",
    "        \"Return exactly ONE of the following strings:\\n\"\n",
    "        \"- search\\n\"\n",
    "        \"- read\\n\"\n",
    "        \"- control\\n\"\n",
    "        \"- chat\\n\\n\"\n",
    "        \"Return ONLY the label.\"\n",
    "    )\n",
    ")\n",
    "# PARAMETER_EXTRACT_PROMPT = SystemMessage(\n",
    "#     content = \"Based on human message, refer which room's lights needs to be controlled and what needs to be done.\"\n",
    "#     f\"The docstring: \\n {control_node.__doc__}\"\n",
    "#     \"Also look at the Rooms pydantic object because you need to fill it in correctly:\"\n",
    "#     f\"{Rooms.schema_json()}\"\n",
    "# )\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Rooms)\n",
    "\n",
    "# TODO: embed config in this prompt as well\n",
    "CONTROL_PROMPT = SystemMessage(\n",
    "    content = (\n",
    "        \"Based on human message, refer which room's lights needs to be controlled and what needs to be done.\"\n",
    "        \"You must output JSON that matches this schema:\"\n",
    "        f\"{parser.get_format_instructions()}\"\n",
    "        \"Do not include any text outside the JSON.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "def classify_intent(state: State):\n",
    "    response = llm.invoke(\n",
    "        [INTENT_PROMPT] + state[\"messages\"]\n",
    "    )\n",
    "\n",
    "    intent = response.content.strip().lower()\n",
    "\n",
    "    return {\n",
    "        \"intent\": intent,\n",
    "        \"messages\": state[\"messages\"] + [response],\n",
    "    }\n",
    "\n",
    "\n",
    "def policy_check_control(state: State):\n",
    "    policy_check = True\n",
    "\n",
    "    return {\n",
    "        \"policy_check\": policy_check,\n",
    "        \"messages\": state[\"messages\"]\n",
    "    }\n",
    "\n",
    "\n",
    "def classify_control(state: State):\n",
    "    response = llm.invoke(\n",
    "        [CONTROL_PROMPT] + state[\"messages\"]\n",
    "    )\n",
    "    rooms = response.content.strip().lower()\n",
    "    print(rooms)\n",
    "    rooms: Rooms = parser.parse(response.content)\n",
    "    return {\n",
    "        \"rooms\": rooms,\n",
    "        \"messages\": state[\"messages\"] + [response]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def route_by_intent(state: State) -> str:\n",
    "    intent = state.get(\"intent\")\n",
    "\n",
    "    if intent == \"read\":\n",
    "        return \"read_node\"\n",
    "    if intent == \"control\":\n",
    "        return \"control_node\"\n",
    "\n",
    "    return \"chat_node\"\n",
    "\n",
    "\n",
    "def read_node(state: State):\n",
    "    print(\"â†’ READ\")\n",
    "    return state\n",
    "\n",
    "def control_parameter_node(state: State):\n",
    "    '''Sends commands for turning on or off lights.\n",
    "    Based on `topics` field in `rooms` parameter, it will decide which mqtt topic\n",
    "    it will send the message. `status` field in `rooms` class is needed for `on` \n",
    "    or `off` command.\n",
    "\n",
    "    Args:\n",
    "        state (State): the state has been being managed by Langgraph.\n",
    "        rooms (Rooms): Rooms class to be parsed in the function to send \n",
    "        necessary messages to proper mqtt topic.\n",
    "\n",
    "    Returns:\n",
    "        state (State): the state has been being managed by Langgraph.\n",
    "    '''\n",
    "    response = llm.invoke(\n",
    "        [CONTROL_PROMPT] + state[\"messages\"]\n",
    "    )\n",
    "    rooms = response.content.strip().lower()\n",
    "    print(rooms)\n",
    "    rooms: Rooms = parser.parse(response.content)\n",
    "    return {\n",
    "        \"rooms\": rooms,\n",
    "        \"messages\": state[\"messages\"] + [response]\n",
    "    }\n",
    "\n",
    "def chat_node(state: State):\n",
    "    print(\"â†’ CHAT\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c34c92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "graph = StateGraph(State)\n",
    "\n",
    "graph.add_edge(START, \"intent\")\n",
    "graph.add_node(\"intent\", classify_intent)\n",
    "graph.add_node(\"read_node\", read_node)\n",
    "graph.add_node(\"control_parameter_node\", control_parameter_node)\n",
    "graph.add_node(\"chat_node\", chat_node)\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"intent\",\n",
    "    route_by_intent,\n",
    "    {\n",
    "        \"read_node\": \"read_node\",\n",
    "        \"control_node\": \"control_parameter_node\",\n",
    "        \"chat_node\": \"chat_node\",\n",
    "    },\n",
    ")\n",
    "\n",
    "graph.add_edge(\"read_node\", END)\n",
    "graph.add_edge(\"control_parameter_node\", END)\n",
    "graph.add_edge(\"chat_node\", END)\n",
    "\n",
    "app = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3ecf0f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the office light to turn on\n",
      "okay, so i need to figure out how to respond to this user's message about controlling the lights in different rooms. the user provided a specific schema that i have to follow strictly. let me break it down.\n",
      "\n",
      "first, the user sent two messages: \"turn off the bedroom lights\" and \"control the office light to turn on.\" each of these needs to be converted into json according to the given schema.\n",
      "\n",
      "looking at the schema, each json object should have three keys: rooms, topics, and status. the rooms can be either \"office\" or \"bedroom,\" topics are specific strings like \"home/lights/office,\" and status is either \"off\" or \"on.\"\n",
      "\n",
      "for the first message, \"turn off the bedroom lights\":\n",
      "- the room is clearly the bedroom.\n",
      "- the topic should match the bedroom, so it's \"home/lights/bedroom.\"\n",
      "- the action is to turn off, so status is \"off.\"\n",
      "\n",
      "putting that together, the json would be {\"rooms\": \"bedroom\", \"topics\": \"home/lights/bedroom\", \"status\": \"off\"}.\n",
      "\n",
      "for the second message, \"control the office light to turn on\":\n",
      "- the room here is the office.\n",
      "- the topic corresponds to the office, so it's \"home/lights/office.\"\n",
      "- the action is to turn on, so status is \"on.\"\n",
      "\n",
      "so that json would be {\"rooms\": \"office\", \"topics\": \"home/lights/office\", \"status\": \"on\"}.\n",
      "\n",
      "i need to make sure each json object only contains these three keys and follows the correct enum values. also, i shouldn't include any extra text outside the json as per the instructions.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "{\"rooms\": \"bedroom\", \"topics\": \"home/lights/bedroom\", \"status\": \"off\"}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "result = app.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Turn off the bedroom lights\")],\n",
    "    \"intent\": None,\n",
    "    \"rooms\": None\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c9c4657a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Turn off the bedroom lights', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='control', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:14B', 'created_at': '2026-01-08T01:00:06.791113Z', 'done': True, 'done_reason': 'stop', 'total_duration': 22920448083, 'load_duration': 76085000, 'prompt_eval_count': 40, 'prompt_eval_duration': 1634748791, 'eval_count': 204, 'eval_duration': 19829456701, 'logprobs': None, 'model_name': 'deepseek-r1:14B', 'model_provider': 'ollama'}, id='lc_run--019b9b1d-db7c-7661-83af-c12b821698cb-0', usage_metadata={'input_tokens': 40, 'output_tokens': 204, 'total_tokens': 244}),\n",
       "  AIMessage(content=' the office light to turn on\\nOkay, so I need to figure out how to respond to this user\\'s message about controlling the lights in different rooms. The user provided a specific schema that I have to follow strictly. Let me break it down.\\n\\nFirst, the user sent two messages: \"Turn off the bedroom lights\" and \"control the office light to turn on.\" Each of these needs to be converted into JSON according to the given schema.\\n\\nLooking at the schema, each JSON object should have three keys: rooms, topics, and status. The rooms can be either \"office\" or \"bedroom,\" topics are specific strings like \"home/lights/office,\" and status is either \"off\" or \"on.\"\\n\\nFor the first message, \"Turn off the bedroom lights\":\\n- The room is clearly the bedroom.\\n- The topic should match the bedroom, so it\\'s \"home/lights/bedroom.\"\\n- The action is to turn off, so status is \"off.\"\\n\\nPutting that together, the JSON would be {\"rooms\": \"bedroom\", \"topics\": \"home/lights/bedroom\", \"status\": \"off\"}.\\n\\nFor the second message, \"control the office light to turn on\":\\n- The room here is the office.\\n- The topic corresponds to the office, so it\\'s \"home/lights/office.\"\\n- The action is to turn on, so status is \"on.\"\\n\\nSo that JSON would be {\"rooms\": \"office\", \"topics\": \"home/lights/office\", \"status\": \"on\"}.\\n\\nI need to make sure each JSON object only contains these three keys and follows the correct enum values. Also, I shouldn\\'t include any extra text outside the JSON as per the instructions.\\n</think>\\n\\n```json\\n{\"rooms\": \"bedroom\", \"topics\": \"home/lights/bedroom\", \"status\": \"off\"}\\n```', additional_kwargs={}, response_metadata={'model': 'deepseek-r1:14B', 'created_at': '2026-01-08T01:00:46.637644Z', 'done': True, 'done_reason': 'stop', 'total_duration': 39832356750, 'load_duration': 51403708, 'prompt_eval_count': 262, 'prompt_eval_duration': 2802483375, 'eval_count': 382, 'eval_duration': 34968388993, 'logprobs': None, 'model_name': 'deepseek-r1:14B', 'model_provider': 'ollama'}, id='lc_run--019b9b1e-3511-7823-9ecf-497fff3a5355-0', usage_metadata={'input_tokens': 262, 'output_tokens': 382, 'total_tokens': 644})],\n",
       " 'intent': 'control',\n",
       " 'rooms': Rooms(rooms='bedroom', topics='home/lights/bedroom', status='off')}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1cbae3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['messages', 'intent', 'rooms'])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93341d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f909f77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fa0366",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef0671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casa (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
